{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "collapsed": true,
        "id": "XifrUdmdjduJ",
        "outputId": "c85573f0-96e6-4043-ad86-fbe30fbedadd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting numpy>=1.23.2 (from pandas)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "e9abcc6fadc949178a9fc746516cba79"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reddit Topic Modeling"
      ],
      "metadata": {
        "id": "qYQeVz8aNIV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gVQ5R63HjAyH",
        "outputId": "28952a7d-0c52-49b7-8197-48a676e78258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     topic    topic_label                                          top_words  \\\n",
            "39    72.0   trump israel  [trump, israel, iran, thought, ceasefire, know...   \n",
            "79   144.0       day hand  [day, hand, baby, two, around, move, may, big,...   \n",
            "58   103.0        car lol  [car, lol, almost, model, ended, found, rest, ...   \n",
            "53    97.0     first time  [first, time, make, favorite, see, series, cha...   \n",
            "84   153.0     made never  [made, never, least, cake, color, coffee, stil...   \n",
            "..     ...            ...                                                ...   \n",
            "104  196.0    source game  [source, game, gt, good, see, really, back, no...   \n",
            "61   116.0       http com  [http, com, original, say, right, post, coming...   \n",
            "34    63.0      even told  [even, told, enough, one, ever, maybe, woman, ...   \n",
            "69   128.0  going article  [going, article, get, go, six, made, dinner, w...   \n",
            "31    57.0     room would  [room, would, cover, like, stay, house, door, ...   \n",
            "\n",
            "     total_engagement  doc_count  \n",
            "39             164605         36  \n",
            "79             129215         11  \n",
            "58             125500          9  \n",
            "53             111000         25  \n",
            "84              80462         14  \n",
            "..                ...        ...  \n",
            "104              1740          1  \n",
            "61               1726          2  \n",
            "34               1250          2  \n",
            "69                817          1  \n",
            "31                658          1  \n",
            "\n",
            "[107 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv('reddit_hot_posts 2.csv', low_memory=False)\n",
        "\n",
        "# 2. Define engagement metric\n",
        "df['engagement'] = df['score'].fillna(0) + df['num_comments'].fillna(0)\n",
        "\n",
        "# 3. NLTK only for stopwords + lemmatizer data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')    # for WordNet lemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    # simple_preprocess: lowercases, strips accents, tokenizes on word boundaries, removes tokens <2 or >15 chars\n",
        "    tokens = simple_preprocess(text, deacc=True)\n",
        "    return [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
        "\n",
        "# Combine title + selftext\n",
        "df['doc'] = (df['title'].fillna('') + ' ' + df['selftext'].fillna('')).apply(preprocess)\n",
        "\n",
        "# 4. Build dictionary & corpus\n",
        "dictionary = corpora.Dictionary(df['doc'])\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df['doc']]\n",
        "\n",
        "# 5. Fit LDA\n",
        "NUM_TOPICS = 200\n",
        "lda = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=NUM_TOPICS,\n",
        "    passes=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 6. Assign dominant topic\n",
        "def get_dominant_topic(bow):\n",
        "    topics = lda.get_document_topics(bow)\n",
        "    return max(topics, key=lambda x: x[1])[0] if topics else None\n",
        "\n",
        "df['topic'] = [get_dominant_topic(b) for b in corpus]\n",
        "\n",
        "# 7. Aggregate engagement\n",
        "agg = (\n",
        "    df.groupby('topic')\n",
        "      .agg(\n",
        "          total_engagement=('engagement', 'sum'),\n",
        "          doc_count=('engagement', 'size')\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# 8. Extract top words per topic\n",
        "agg['top_words'] = agg['topic'].apply(lambda t: [w for w, _ in lda.show_topic(int(t), topn=10)])\n",
        "\n",
        "# … after you’ve built `agg` and populated `agg['top_words']` …\n",
        "\n",
        "# 9a. Auto‐generate a simple label from the first two top words\n",
        "agg['topic_label'] = agg['top_words'].apply(lambda words: ' '.join(words[:2]))\n",
        "\n",
        "# 9b. Pick top 200 topics by engagement\n",
        "top200 = (\n",
        "    agg\n",
        "    .sort_values('total_engagement', ascending=False)\n",
        "    .head(200)\n",
        "    # reorder columns for clarity\n",
        "    [['topic', 'topic_label', 'top_words', 'total_engagement', 'doc_count']]\n",
        ")\n",
        "\n",
        "# 10. Save or inspect\n",
        "top200.to_csv('top_200_topics_labeled.csv', index=False)\n",
        "print(top200)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Youtube Topic Modeling"
      ],
      "metadata": {
        "id": "WtaTU_8oNN_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "\n",
        "# 1. Load YouTube data\n",
        "df = pd.read_csv('youtube_trending_analysis_2025-06-20 (2).csv', low_memory=False)\n",
        "\n",
        "# 2. Engagement = View Count + Like Count\n",
        "df['engagement'] = (\n",
        "    df['View Count'].fillna(0)\n",
        "  + df['Like Count'].fillna(0)\n",
        ")\n",
        "\n",
        "# 3. Download NLTK resources (if you haven’t already)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    tokens = simple_preprocess(text, deacc=True)\n",
        "    return [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
        "\n",
        "# 4. Build “documents” from Title + Description\n",
        "#    (make sure your file has a 'Description' column—if it’s named differently, swap in the exact name)\n",
        "df['doc'] = (\n",
        "    df['Title'].fillna('') + ' '\n",
        "  + df['Description'].fillna('')\n",
        ").apply(preprocess)\n",
        "\n",
        "# 5. Dictionary & Corpus\n",
        "dictionary = corpora.Dictionary(df['doc'])\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df['doc']]\n",
        "\n",
        "# 6. Fit LDA\n",
        "NUM_TOPICS = 100\n",
        "lda = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=NUM_TOPICS,\n",
        "    passes=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 7. Dominant topic per video\n",
        "def get_dominant_topic(bow):\n",
        "    topics = lda.get_document_topics(bow)\n",
        "    return max(topics, key=lambda x: x[1])[0] if topics else None\n",
        "\n",
        "df['topic'] = [get_dominant_topic(b) for b in corpus]\n",
        "\n",
        "# 8. Aggregate engagement by topic\n",
        "agg = (\n",
        "    df.groupby('topic')\n",
        "      .agg(\n",
        "          total_engagement=('engagement', 'sum'),\n",
        "          video_count=('engagement', 'size')\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# 9. Extract top words & naive labels\n",
        "agg['top_words'] = agg['topic'].apply(\n",
        "    lambda t: [w for w, _ in lda.show_topic(int(t), topn=10)]\n",
        ")\n",
        "agg['topic_label'] = agg['top_words'].apply(lambda ws: ' '.join(ws[:2]))\n",
        "\n",
        "# 10. Select top topics\n",
        "top_topics = (\n",
        "    agg.sort_values('total_engagement', ascending=False)\n",
        "       .head(min(200, NUM_TOPICS))\n",
        "       [['topic','topic_label','top_words','total_engagement','video_count']]\n",
        ")\n",
        "\n",
        "# 11. Save & inspect\n",
        "top_topics.to_csv('youtube_top_topics.csv', index=False)\n",
        "print(top_topics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FgHnoJAi7HRe",
        "outputId": "78cc7afb-fcbe-49c0-c399-5a903b5db8e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    topic         topic_label  \\\n",
            "20   20.0         枚ランタム封入 種より   \n",
            "67   68.0      official music   \n",
            "48   49.0      official ateez   \n",
            "63   64.0     youtube twitter   \n",
            "60   61.0      music director   \n",
            "..    ...                 ...   \n",
            "7     7.0            nbc news   \n",
            "24   24.0          gd peacock   \n",
            "85   87.0  bensonboone benson   \n",
            "77   79.0               कर ki   \n",
            "14   14.0           sky sport   \n",
            "\n",
            "                                            top_words  total_engagement  \\\n",
            "20  [枚ランタム封入, 種より, japan, cd, book, photo, 枚封入, of...         333009128   \n",
            "67  [official, music, song, sidhu, punjabi, apple,...         211752939   \n",
            "48  [official, ateez, kqent, ojo, osaka, kq, faceb...         194464050   \n",
            "63  [youtube, twitter, jp, channel, co, tiktok, su...         129678402   \n",
            "60  [music, director, producer, song, production, ...         128813574   \n",
            "..                                                ...               ...   \n",
            "7   [nbc, news, wjar, u, follow, app, read, karen,...           1370891   \n",
            "24  [gd, peacock, key, answer, score, result, card...           1356792   \n",
            "85  [bensonboone, benson, boone, video, blue, yout...           1315941   \n",
            "77  [कर, ki, contact, licensing, john, video, cour...            731980   \n",
            "14  [sky, sport, bit, ly, india, test, highlight, ...            485716   \n",
            "\n",
            "    video_count  \n",
            "20            8  \n",
            "67           19  \n",
            "48           12  \n",
            "63           83  \n",
            "60           22  \n",
            "..          ...  \n",
            "7             7  \n",
            "24            9  \n",
            "85            6  \n",
            "77            4  \n",
            "14            1  \n",
            "\n",
            "[98 rows x 5 columns]\n"
          ]
        }
      ]
    }
  ]
}
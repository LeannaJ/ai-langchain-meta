{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y53R6-GnDwla"
      },
      "outputs": [],
      "source": [
        "### Common Source Code\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import json\n",
        "\n",
        "import re\n",
        "\n",
        "_topic_cache = {}\n",
        "\n",
        "_TONE_TEMPLATES = {\n",
        "    \"neutral\":           \"Write a factual, objective summary of the topic.\",\n",
        "    \"informative\":       \"Explain the topic clearly and concisely, focusing on key facts.\",\n",
        "    \"serious\":           \"Discuss the topic in a respectful, serious tone highlighting importance.\",\n",
        "    \"playful\":           \"Write in a lighthearted, playful voice, using fun wordplay.\",\n",
        "    \"humorous\":          \"Craft a witty, amusing one-liner.\",\n",
        "    \"witty\":             \"Compose a sharp, clever one-liner.\",\n",
        "    \"empathetic\":        \"Acknowledge feelings with empathy and support.\",\n",
        "    \"inspirational\":     \"Create an uplifting, motivational message.\",\n",
        "    \"motivational\":      \"Write a caption that energizes and prompts action.\",\n",
        "    \"supportive\":        \"Offer reassurance and encouragement.\",\n",
        "    \"suspenseful\":       \"Build curiosity with a suspenseful teaser.\",\n",
        "    \"reflective\":        \"Invite thoughtful introspection.\",\n",
        "    \"poetic\":            \"Use vivid imagery and metaphor in lyrical form.\",\n",
        "    \"analytical\":        \"Provide an analytical breakdown with key insights.\",\n",
        "    \"critical\":          \"Present a balanced critique, noting pros and cons.\",\n",
        "    \"descriptive\":       \"Paint a vivid picture with sensory details.\",\n",
        "    \"conversational\":    \"Write casually, as if speaking with a friend.\",\n",
        "    \"authoritative\":     \"Demonstrate expertise with confident tone.\",\n",
        "    \"educational\":       \"Simplify complex ideas into clear educational language.\",\n",
        "    \"promotional\":       \"Highlight benefits and encourage engagement.\",\n",
        "    \"nostalgic\":         \"Evoke fond memories and a sense of nostalgia.\",\n",
        "    \"inquisitive\":       \"Pose an engaging question to invite responses.\",\n",
        "    \"urgent\":            \"Write a call-to-action prompting immediate attention.\",\n",
        "    \"cautionary\":        \"Warn about risks or pitfalls in a cautionary style.\",\n",
        "    \"celebratory\":       \"Express enthusiasm and celebration.\",\n",
        "    \"sarcastic\":         \"Convey the message with ironic, tongue-in-cheek humor.\",\n",
        "    \"optimistic\":        \"Focus on positive outcomes and hopeful possibilities.\",\n",
        "    \"skeptical\":         \"Question assumptions and express healthy doubt.\",\n",
        "    \"minimalist\":        \"Say as much as possible with as few words as necessary.\",\n",
        "    \"dramatic\":          \"Heighten stakes and emotion with theatrical flair.\",\n",
        "    \"mysterious\":        \"Hint at more beneath the surface; keep it intriguing and slightly vague.\",\n",
        "    \"friendly\":          \"Sound warm, approachable, and casual like a trusted peer.\",\n",
        "    \"empowering\":        \"Give the audience a sense of agency and strength.\",\n",
        "    \"edgy\":              \"Take a bold, slightly provocative stance to stand out.\",\n",
        "    \"exclusive\":         \"Make the audience feel like they're getting insider access.\",\n",
        "    \"trendsetting\":      \"Frame it as ahead-of-the-curve and culturally leading.\",\n",
        "    \"reassuring\":        \"Calm concerns and reinforce trust gently.\",\n",
        "    \"confident\":         \"State points decisively with no hesitation.\",\n",
        "    \"behind-the-scenes\": \"Reveal insider context in a candid, informal way.\",\n",
        "}\n",
        "\n",
        "_ALLOWED_GENRES = [\n",
        "    \"news\", \"breaking_news\", \"entertainment\", \"movies\", \"music\", \"sports\", \"tech\",\n",
        "    \"artificial_intelligence\", \"lifestyle\", \"health\", \"wellness\", \"fitness\",\n",
        "    \"business\", \"startups\", \"finance\", \"personal_finance\", \"crypto\", \"real_estate\",\n",
        "    \"arts\", \"culture\", \"community\", \"education\", \"science\", \"environment\",\n",
        "    \"sustainability\", \"travel\", \"food\", \"gaming\", \"fashion\", \"beauty\",\n",
        "    \"relationships\", \"parenting\", \"self_help\", \"spirituality\", \"automotive\",\n",
        "    \"DIY\", \"pets\", \"career\", \"politics\", \"policy\", \"local\", \"global\", \"niche\",\n",
        "    \"events\", \"celebrity\", \"trends\", \"memes\", \"innovation\", \"social_justice\"\n",
        "]\n",
        "\n",
        "_ALLOWED_FORMATS = [\n",
        "    \"listicle\", \"how-to\", \"tutorial\", \"question\", \"trivia\", \"quote\", \"narrative\",\n",
        "    \"story\", \"thread\", \"comparison\", \"reaction\", \"news-flash\", \"behind-the-scenes\",\n",
        "    \"interview\", \"faq\", \"checklist\", \"myth-busting\", \"before-after\", \"survey\",\n",
        "    \"poll\", \"prediction\", \"countdown\", \"tips\", \"case_study\", \"testimonial\",\n",
        "    \"mini-review\", \"challenge\", \"announcement\", \"event_recap\", \"opinion\",\n",
        "    \"analysis\", \"explainer\", \"warning\", \"celebration\", \"reminder\", \"spotlight\",\n",
        "    \"guide\", \"hack\", \"projection\", \"quote-card\"\n",
        "]\n",
        "\n",
        "_ALLOWED_AUDIENCES = [\n",
        "    \"general_public\", \"influencers\", \"local_community\", \"entrepreneurs\",\n",
        "    \"students\", \"creators\", \"early_adopters\", \"parents\", \"professionals\",\n",
        "    \"investors\", \"gamers\", \"travelers\", \"foodies\", \"fitness_enthusiasts\",\n",
        "    \"fashionistas\", \"pet_owners\", \"eco-conscious\", \"diyers\", \"researchers\",\n",
        "    \"local_businesses\", \"tourists\", \"educators\", \"tech_enthusiasts\", \"career_seekers\",\n",
        "    \"content_marketers\", \"nonprofit_leaders\", \"policy_makers\", \"health_advocates\"\n",
        "]\n",
        "\n",
        "def _normalize_list(lst):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in lst:\n",
        "        if not isinstance(x, str):\n",
        "            continue\n",
        "        key = x.strip().lower()\n",
        "        if key and key not in seen:\n",
        "            seen.add(key)\n",
        "            out.append(key)\n",
        "    return out\n",
        "\n",
        "def _validate_taxonomy(genre, tone, fmt, audience):\n",
        "    # genre: list, tone: string, fmt: string, audience: list\n",
        "    validated = {}\n",
        "    # genres: intersect with allowed, take up to 3\n",
        "    genres = [g for g in genre if isinstance(g, str)]\n",
        "    genres = [g.lower() for g in genres]\n",
        "    genres = [g for g in genres if g in _ALLOWED_GENRES]\n",
        "    validated[\"genre\"] = genres[:3] if genres else []\n",
        "    # tone: fallback to neutral if unknown\n",
        "    t = tone.lower() if isinstance(tone, str) else \"\"\n",
        "    validated[\"tone\"] = t if t in _TONE_TEMPLATES else \"neutral\"\n",
        "    # format\n",
        "    f = fmt.lower() if isinstance(fmt, str) else \"\"\n",
        "    validated[\"format\"] = f if f in _ALLOWED_FORMATS else \"listicle\"\n",
        "    # audience\n",
        "    auds = [a for a in audience if isinstance(a, str)]\n",
        "    auds = [a.lower() for a in auds]\n",
        "    auds = [a for a in auds if a in _ALLOWED_AUDIENCES]\n",
        "    validated[\"audience\"] = auds[:3] if auds else [\"general_public\"]\n",
        "    return validated\n",
        "\n",
        "def _validate_captions_structure(captions):\n",
        "    if not isinstance(captions, list):\n",
        "        return False\n",
        "    for c in captions:\n",
        "        if not isinstance(c, str):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def safe_parse_json(raw_text, description, client=None):\n",
        "    try:\n",
        "        return json.loads(raw_text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        if client:\n",
        "            client.stream_message(f\"‚ö†Ô∏è Failed to parse {description} JSON directly: {e}. Attempting recovery‚Ä¶\\n\")\n",
        "            client.stream_message(f\"Raw output was: {raw_text[:1000]}{'...(truncated)' if len(raw_text) > 1000 else ''}\\n\")\n",
        "        # Attempt to extract the first JSON object substring (handles nested braces)\n",
        "        try:\n",
        "            # Recursive regex using balancing is not natively supported; simple fallback: find first {...}\n",
        "            match = re.search(r\"\\{[^{}]*\\}\", raw_text)\n",
        "            if match:\n",
        "                try:\n",
        "                    return json.loads(match.group(0))\n",
        "                except Exception as e2:\n",
        "                    if client:\n",
        "                        client.stream_message(f\"‚ö†Ô∏è Recovery attempt for {description} also failed: {e2}\\n\")\n",
        "        except re.error:\n",
        "            pass\n",
        "        return {}\n",
        "\n",
        "\n",
        "\n",
        "### Workflow Source Code\n",
        "\n",
        "### <CaptionGen node source code>\n",
        "def caption_generator(topic):\n",
        "    \"\"\"\n",
        "    Generate context-aware, multi-genre/tone/format/audience Instagram captions for a topic,\n",
        "    caching with 24h TTL, and defensive coding.\n",
        "    \"\"\"\n",
        "    from abacusai import ApiClient, AgentResponse, Blob\n",
        "\n",
        "    client = ApiClient()\n",
        "    now = datetime.utcnow()\n",
        "    ttl = timedelta(hours=24)\n",
        "\n",
        "    # 1) Prepare research\n",
        "    search_queries = [\n",
        "        f\"{topic}\"\n",
        "        f\"{topic} latest news\",\n",
        "        f\"{topic} trending\",\n",
        "        f\"{topic} instagram posts\",\n",
        "    ]\n",
        "\n",
        "    # 2) Cache lookup\n",
        "    cache = _topic_cache.get(topic)\n",
        "    if cache and (now - cache['cached_at'] < ttl):\n",
        "        client.stream_message(f\"‚ôªÔ∏è Using cached results (age {(now - cache['cached_at']).total_seconds()/3600:.1f}h)\\n\")\n",
        "        genre_list    = cache['genre']\n",
        "        tone_label    = cache['tone']\n",
        "        format_label  = cache['format']\n",
        "        audience_list = cache['audience']\n",
        "        summary       = cache['summary']\n",
        "        captions      = cache['captions']\n",
        "    else:\n",
        "        # 3) Research\n",
        "        client.stream_message(f\"üîç Researching topic: {topic}...\\n\")\n",
        "        snippets = []\n",
        "        for q in search_queries:\n",
        "            try:\n",
        "                resp = client.search_web_for_llm(queries=[q], fetch_content=True, max_results=2)\n",
        "                for r in getattr(resp, 'search_results', []):\n",
        "                    content = getattr(r, 'content', '') or getattr(r, 'snippet', '') or getattr(r, 'title', '')\n",
        "                    if content:\n",
        "                        snippets.append(content)\n",
        "            except Exception as e:\n",
        "                client.stream_message(f\"‚ö†Ô∏è Search error for '{q}': {e}\\n\")\n",
        "        combined = ' '.join(snippets[:6]) or f\"General information about {topic}\"\n",
        "\n",
        "        # 4) Topic taxonomy classification\n",
        "        client.stream_message(\"üóÇ Classifying topic taxonomy with expanded options...\\n\")\n",
        "        try:\n",
        "            taxonomy_schema = {\n",
        "                \"genre\": {\n",
        "                    \"type\": \"list\",\n",
        "                    \"description\": \"List of up to 3 genres relevant to the topic.\",\n",
        "                    \"is_required\": True,\n",
        "                },\n",
        "                \"tone\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The primary tonal style to use.\",\n",
        "                    \"is_required\": True,\n",
        "                },\n",
        "                \"format\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The caption format or structure.\",\n",
        "                    \"is_required\": True,\n",
        "                },\n",
        "                \"audience\": {\n",
        "                    \"type\": \"list\",\n",
        "                    \"description\": \"Target audience segments.\",\n",
        "                    \"is_required\": True,\n",
        "                },\n",
        "            }\n",
        "\n",
        "            system_msg = f\"\"\"\n",
        "Classify the topic into the following JSON schema. Use up to three entries for list fields.\n",
        "Allowed genres: {', '.join(_ALLOWED_GENRES)}.\n",
        "Allowed tones: any of the keys from the tone templates: {', '.join(_TONE_TEMPLATES.keys())}.\n",
        "Allowed formats: {', '.join(_ALLOWED_FORMATS)}.\n",
        "Allowed audiences: {', '.join(_ALLOWED_AUDIENCES)}.\n",
        "Provide the most relevant items based on the research snippet.\n",
        "Output ONLY valid JSON with keys: genre, tone, format, audience. Do not include any explanation or extra fields.\n",
        "\"\"\"\n",
        "\n",
        "            tax_resp = client.evaluate_prompt(\n",
        "                prompt=f\"Research: {combined[:1000]}\",\n",
        "                system_message=system_msg,\n",
        "                llm_name=\"GEMINI_2_FLASH\",\n",
        "                response_type=\"json\",\n",
        "                json_response_schema=taxonomy_schema,\n",
        "                max_tokens=300\n",
        "            )\n",
        "            raw_tax = safe_parse_json(getattr(tax_resp, \"content\", \"\") or \"\", \"taxonomy\", client)\n",
        "            validated = _validate_taxonomy(\n",
        "                genre=raw_tax.get('genre', []),\n",
        "                tone=raw_tax.get('tone', 'neutral'),\n",
        "                fmt=raw_tax.get('format', 'listicle'),\n",
        "                audience=raw_tax.get('audience', [])\n",
        "            )\n",
        "            genre_list    = validated['genre']\n",
        "            tone_label    = validated['tone']\n",
        "            format_label  = validated['format']\n",
        "            audience_list = validated['audience']\n",
        "        except Exception as e:\n",
        "            client.stream_message(f\"‚ö†Ô∏è Taxonomy classification failed: {e}\\n\")\n",
        "            # fallback defaults\n",
        "            genre_list, tone_label, format_label, audience_list = [\"niche\"], \"neutral\", \"listicle\", [\"general_public\"]\n",
        "\n",
        "        # Enforce neutrality if politics or policy appears\n",
        "        if any(g in {\"politics\", \"policy\"} for g in genre_list):\n",
        "            tone_label = \"neutral\"\n",
        "\n",
        "        # 5) Summary + captions generation\n",
        "        client.stream_message(\"‚úÇÔ∏è Generating summary and captions with diversity...\\n\")\n",
        "        try:\n",
        "            summary_captions_schema = {\n",
        "                \"summary\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A concise summary of the topic suitable for social media captioning.\",\n",
        "                    \"is_required\": True,\n",
        "                },\n",
        "                \"captions\": {\n",
        "                    \"type\": \"list\",\n",
        "                    \"description\": \"A list of suggested captions for Instagram posts.\",\n",
        "                    \"is_required\": True,\n",
        "                },\n",
        "            }\n",
        "\n",
        "            prompt_txt = (\n",
        "                f\"Topic: {topic}\\n\"\n",
        "                f\"Research summary: {combined[:800]}\\n\"\n",
        "                f\"Genres: {', '.join(genre_list)}\\n\"\n",
        "                f\"Tone: {tone_label}\\n\"\n",
        "                f\"Format: {format_label}\\n\"\n",
        "                f\"Audience: {', '.join(audience_list)}\"\n",
        "            )\n",
        "            sys_msg = (\n",
        "                _TONE_TEMPLATES.get(tone_label, _TONE_TEMPLATES['neutral']) +\n",
        "                f\" Generate **exactly five** unique captions (no more, no fewer) in {format_label} format, tailored for {', '.join(audience_list)}. \"\n",
        "                \"Each caption should include relevant hashtags and optionally emojis. Vary phrasing so they feel distinct. \"\n",
        "                \"Output ONLY valid JSON with keys 'summary' and 'captions' and no extra commentary.\"\n",
        "            )\n",
        "\n",
        "            def _unique_captions(captions_list):\n",
        "                seen = set()\n",
        "                unique = []\n",
        "                for c in captions_list:\n",
        "                    if not isinstance(c, str):\n",
        "                        continue\n",
        "                    cleaned = c.strip()\n",
        "                    key = cleaned.lower()\n",
        "                    if key and key not in seen:\n",
        "                        seen.add(key)\n",
        "                        unique.append(cleaned)\n",
        "                return unique\n",
        "\n",
        "            # ‚îÄ‚îÄ NEW: ask Gemini and retry up to 3 times until we have ‚â•5 captions ‚îÄ‚îÄ\n",
        "            def _ask_gemini_once(system_message):\n",
        "                resp = client.evaluate_prompt(\n",
        "                    prompt=prompt_txt,\n",
        "                    system_message=system_message,\n",
        "                    llm_name=\"GEMINI_2_FLASH\",\n",
        "                    response_type=\"json\",\n",
        "                    json_response_schema=summary_captions_schema,\n",
        "                    max_tokens=700\n",
        "                )\n",
        "                return safe_parse_json(getattr(resp, \"content\", \"\") or \"\", \"summary+captions\", client)\n",
        "\n",
        "            summary = \"\"\n",
        "            captions = []\n",
        "            for attempt in range(1, 4):  # tries 1-3\n",
        "                try:\n",
        "                    data = _ask_gemini_once(sys_msg)\n",
        "                    summary = data.get('summary', '').strip()\n",
        "                    raw_captions = data.get('captions', [])\n",
        "                    captions = _unique_captions(raw_captions)\n",
        "                    if len(captions) >= 5:\n",
        "                        captions = captions[:5]\n",
        "                        break  # success\n",
        "                    client.stream_message(\n",
        "                        f\"‚ö†Ô∏è Gemini returned {len(captions)} caption(s) (try {attempt}/3). Retrying‚Ä¶\\n\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    client.stream_message(f\"‚ö†Ô∏è Gemini error (try {attempt}/3): {e}\\n\")\n",
        "                    # on malformed JSON, optionally retry with stricter reminder\n",
        "                    if attempt == 1:\n",
        "                        sys_msg = sys_msg + \" AGAIN: Output only the JSON object, no explanation.\"\n",
        "            # final safeguard ‚Äì pad to five if still short\n",
        "            if len(captions) < 5:\n",
        "                client.stream_message(\"‚ö†Ô∏è Using fallback captions to reach five.\\n\")\n",
        "                base = f\"{topic} is trending‚Äîstay tuned!\"\n",
        "                while len(captions) < 5:\n",
        "                    captions.append(base)\n",
        "        except Exception as e:\n",
        "            client.stream_message(f\"‚ö†Ô∏è Summary+captions failed: {e}\\n\")\n",
        "            summary, captions = f\"{topic} is trending.\", [f\"{topic} is trending‚Äîstay tuned!\"]\n",
        "\n",
        "        # 6) Cache results\n",
        "        _topic_cache[topic] = {\n",
        "            \"genre\":    genre_list,\n",
        "            \"tone\":     tone_label,\n",
        "            \"format\":   format_label,\n",
        "            \"audience\": audience_list,\n",
        "            \"summary\":  summary,\n",
        "            \"captions\": captions,\n",
        "            \"cached_at\": now\n",
        "        }\n",
        "\n",
        "    # 7) Build and return response\n",
        "    client.stream_message(\"üì¶ Creating output files...\\n\")\n",
        "    markdown_lines = [\n",
        "        f\"# Caption Generation Report: {topic}\",\n",
        "        \"\",\n",
        "        f\"**Genres:** {', '.join(genre_list)}\",\n",
        "        f\"**Tone:** {tone_label}\",\n",
        "        f\"**Format:** {format_label}\",\n",
        "        f\"**Audience:** {', '.join(audience_list)}\",\n",
        "        \"\",\n",
        "        \"## Summary\",\n",
        "        summary or \"(no summary generated)\",\n",
        "        \"\",\n",
        "        \"## Captions\"\n",
        "    ]\n",
        "    for idx, cap in enumerate(captions, 1):\n",
        "        markdown_lines += [f\"### Caption {idx}\", cap.strip(), \"\"]\n",
        "    markdown_lines += [\n",
        "        \"## Research Queries\",\n",
        "        f\"{', '.join(search_queries)}\"\n",
        "    ]\n",
        "    md_content = \"\\n\".join(markdown_lines)\n",
        "    md_bytes   = md_content.encode(\"utf-8\")\n",
        "    json_blob  = {\n",
        "        \"topic\":     topic,\n",
        "        \"genre\":     genre_list,\n",
        "        \"tone\":      tone_label,\n",
        "        \"format\":    format_label,\n",
        "        \"audience\":  audience_list,\n",
        "        \"summary\":   summary,\n",
        "        \"captions\":  captions,\n",
        "        \"cached_at\": now.isoformat()\n",
        "    }\n",
        "    json_bytes = json.dumps(json_blob, indent=2).encode(\"utf-8\")\n",
        "\n",
        "    return AgentResponse(\n",
        "        summary=summary,\n",
        "        sentiment=tone_label,\n",
        "        captions=captions,\n",
        "        topic=topic,\n",
        "        markdown_file=Blob(\n",
        "            contents=md_bytes,\n",
        "            mime_type=\"text/markdown\",\n",
        "            filename=f\"captiongen_report_{topic.replace(' ', '_')}.md\"\n",
        "        ),\n",
        "        json_file=Blob(\n",
        "            contents=json_bytes,\n",
        "            mime_type=\"application/json\",\n",
        "            filename=f\"captiongen_data_{topic.replace(' ', '_')}.json\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "### <ImageGen node source code>\n",
        "def image_generator_corrected_fixed(captions, sentiment, summary, topic):\n",
        "    \"\"\"\n",
        "    Generate viral Instagram-style images conditioned on individual captions.\n",
        "    Uses Hugging Face InferenceClient with provider=\"replicate\" to call Stable Diffusion 3.5 Large (and fallbacks).\n",
        "    One image per caption (5 captions = 5 images total).\n",
        "    Robust JSON parsing, strict schema enforcement, and retry logic for malformed LLM outputs.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import os\n",
        "    import time\n",
        "    import re\n",
        "    from abacusai import ApiClient, AgentResponse, Blob\n",
        "\n",
        "    # ‚ñ∏‚ñ∏ Coerce captions from JSON string if needed\n",
        "    if isinstance(captions, str):\n",
        "        try:\n",
        "            captions = json.loads(captions)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not isinstance(captions, list) or len(captions) == 0:\n",
        "        captions = [f\"{topic} is trending on Instagram\"] * 5\n",
        "\n",
        "    # Helper for resilient JSON parsing\n",
        "    def safe_parse_json(raw_text, description, client=None):\n",
        "        try:\n",
        "            return json.loads(raw_text)\n",
        "        except json.JSONDecodeError as e:\n",
        "            if client:\n",
        "                client.stream_message(f\"‚ö†Ô∏è Failed to parse {description} JSON directly: {e}. Attempting recovery‚Ä¶\\n\")\n",
        "                client.stream_message(f\"Raw output: {raw_text[:1500]}{'...(truncated)' if len(raw_text) > 1500 else ''}\\n\")\n",
        "            candidates = []\n",
        "            # Simple first {...} block\n",
        "            m1 = re.search(r\"\\{[^{}]*\\}\", raw_text)\n",
        "            if m1:\n",
        "                candidates.append(m1.group(0))\n",
        "            # Balanced-brace heuristic\n",
        "            stack = []\n",
        "            start_idx = None\n",
        "            for i, ch in enumerate(raw_text):\n",
        "                if ch == \"{\":\n",
        "                    if start_idx is None:\n",
        "                        start_idx = i\n",
        "                    stack.append(\"{\")\n",
        "                elif ch == \"}\":\n",
        "                    if stack:\n",
        "                        stack.pop()\n",
        "                        if not stack and start_idx is not None:\n",
        "                            candidates.append(raw_text[start_idx : i + 1])\n",
        "                            break\n",
        "            for cand in candidates:\n",
        "                try:\n",
        "                    return json.loads(cand)\n",
        "                except Exception as e2:\n",
        "                    if client:\n",
        "                        client.stream_message(f\"‚ö†Ô∏è Recovery candidate failed for {description}: {e2}\\n\")\n",
        "            return {}\n",
        "\n",
        "    client = ApiClient()\n",
        "    client.stream_message(f\"üöÄ Starting caption-conditioned image generation for topic: {topic}\\n\")\n",
        "\n",
        "    # ---- Secrets ----\n",
        "    hf_api_key = None\n",
        "    try:\n",
        "        hf_api_key = client.get_organization_secret(secret_key=\"HUGGINGFACE_API_KEY\").value\n",
        "        client.stream_message(\"‚úÖ Retrieved Hugging Face API key.\\n\")\n",
        "    except Exception as e:\n",
        "        client.stream_message(f\"‚ö†Ô∏è Could not retrieve Hugging Face API key: {e}\\n\")\n",
        "\n",
        "    # ---- Sentiment-based style guide ----\n",
        "    sentiment_styles = {\n",
        "        \"tragic\": \"dramatic lighting, emotional composition, soft shadows, muted tones, photorealistic\",\n",
        "        \"happy\": \"bright vibrant lighting, joyful atmosphere, golden hour glow, photorealistic\",\n",
        "        \"political\": \"documentary editorial style, clean composition, balanced lighting, professional photorealism\",\n",
        "        \"funny\": \"playful expressions, saturated colors, whimsical props, high energy, photorealistic\",\n",
        "        \"pop-culture\": \"stylish modern aesthetic, trendy filters, influencer vibe, photorealistic\",\n",
        "        \"offensive\": \"informative layout, neutral lighting, respectful tonality, photorealistic\",\n",
        "        \"educational\": \"clean academic aesthetic, professional lighting, campus atmosphere, photorealistic\",\n",
        "        \"inspirational\": \"uplifting composition, golden hour, aspirational mood, photorealistic\",\n",
        "        \"analytical\": \"infographic style elements, data visualization hints, modern tech aesthetic, photorealistic\",\n",
        "        \"nostalgic\": \"warm vintage tones, soft focus edges, memory-like quality, photorealistic\",\n",
        "        \"mysterious\": \"moody lighting, intriguing shadows, cinematic atmosphere, photorealistic\",\n",
        "        \"celebratory\": \"festive colors, dynamic energy, joyful composition, photorealistic\"\n",
        "    }\n",
        "    style_guide = sentiment_styles.get(\n",
        "        (sentiment or \"\").lower(),\n",
        "        \"professional photography, engaging composition, Instagram-optimized, photorealistic\",\n",
        "    )\n",
        "\n",
        "    # Attempt to import HF client\n",
        "    try:\n",
        "        from huggingface_hub import InferenceClient\n",
        "    except ImportError:\n",
        "        InferenceClient = None  # fallback to REST\n",
        "\n",
        "    def call_hf_diffusion(prompt, negative_prompt, guidance_scale, steps):\n",
        "        primary_model = os.getenv(\"HF_MODEL_REPO\", \"stabilityai/stable-diffusion-3.5-large\")\n",
        "        fallbacks = [\"stabilityai/sdxl-turbo\", \"stabilityai/stable-diffusion-xl-base-1.0\"]\n",
        "        tried = []\n",
        "\n",
        "        if not hf_api_key:\n",
        "            raise RuntimeError(\"No Hugging Face API key available for inference call.\")\n",
        "\n",
        "        client.stream_message(f\"‚ÑπÔ∏è Primary HF model resolved to '{primary_model}'. Fallbacks: {fallbacks}\\n\")\n",
        "\n",
        "        if InferenceClient is not None:\n",
        "            try:\n",
        "                hf = InferenceClient(provider=\"replicate\", api_key=hf_api_key, timeout=300)\n",
        "                client.stream_message(f\"üß™ Using HuggingFace InferenceClient (replicate) for model '{primary_model}'\\n\")\n",
        "                raw = hf.text_to_image(\n",
        "                    prompt,\n",
        "                    model=primary_model,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    num_inference_steps=steps,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                )\n",
        "                # Normalize outputs\n",
        "                if isinstance(raw, (bytes, bytearray)):\n",
        "                    return bytes(raw)\n",
        "                try:\n",
        "                    from PIL import Image\n",
        "                    import io\n",
        "                    if isinstance(raw, Image.Image):\n",
        "                        buf = io.BytesIO()\n",
        "                        raw.save(buf, format=\"PNG\")\n",
        "                        return buf.getvalue()\n",
        "                except ImportError:\n",
        "                    pass\n",
        "                import io\n",
        "                if isinstance(raw, dict):\n",
        "                    img = raw.get(\"image\") or raw.get(\"images\") or None\n",
        "                    if isinstance(img, list) and img:\n",
        "                        img = img[0]\n",
        "                    if isinstance(img, (bytes, bytearray)):\n",
        "                        return bytes(img)\n",
        "                    try:\n",
        "                        from PIL import Image\n",
        "                        if isinstance(img, Image.Image):\n",
        "                            buf = io.BytesIO()\n",
        "                            img.save(buf, format=\"PNG\")\n",
        "                            return buf.getvalue()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if isinstance(raw, str):\n",
        "                    import base64\n",
        "                    try:\n",
        "                        return base64.b64decode(raw)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                client.stream_message(f\"‚ö†Ô∏è Unexpected return type from InferenceClient for model '{primary_model}': {type(raw)}. Falling back.\\n\")\n",
        "                tried.append((primary_model, f\"unexpected return type {type(raw)}\"))\n",
        "            except Exception as e:\n",
        "                client.stream_message(f\"‚ö†Ô∏è InferenceClient (replicate) call failed for '{primary_model}': {e}\\n\")\n",
        "                tried.append((primary_model, f\"replicate-client error: {e}\"))\n",
        "\n",
        "        # REST fallback\n",
        "        import requests\n",
        "        for model in [primary_model] + fallbacks:\n",
        "            hf_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "            headers = {\"Authorization\": f\"Bearer {hf_api_key}\"}\n",
        "            payload = {\n",
        "                \"inputs\": prompt,\n",
        "                \"parameters\": {\n",
        "                    \"negative_prompt\": negative_prompt,\n",
        "                    \"guidance_scale\": guidance_scale,\n",
        "                    \"num_inference_steps\": steps,\n",
        "                },\n",
        "                \"options\": {\"wait_for_model\": True},\n",
        "            }\n",
        "            try:\n",
        "                resp = requests.post(hf_url, headers=headers, json=payload, timeout=180)\n",
        "            except Exception as e:\n",
        "                client.stream_message(f\"‚ö†Ô∏è Network error calling HF model '{model}': {e}\\n\")\n",
        "                tried.append((model, f\"network error: {e}\"))\n",
        "                continue\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "                if model != primary_model:\n",
        "                    client.stream_message(f\"‚ÑπÔ∏è Fell back to model '{model}' after primary '{primary_model}' failed.\\n\")\n",
        "                return resp.content\n",
        "\n",
        "            body_preview = resp.text if len(resp.text) < 1000 else resp.text[:1000] + \"...(truncated)\"\n",
        "            client.stream_message(\n",
        "                f\"‚ùå HF inference failed for model '{model}': status={resp.status_code}, body={body_preview}\\n\"\n",
        "            )\n",
        "            tried.append((model, f\"status {resp.status_code}\"))\n",
        "\n",
        "        raise RuntimeError(f\"All HF models failed: {tried}\")\n",
        "\n",
        "    # ---- Negative prompt generation with robust parsing ----\n",
        "    client.stream_message(\"üß† Generating global negative prompt...\\n\")\n",
        "    default_negative = \"blurry, deformed, watermark, low resolution, bad anatomy, oversaturated\"\n",
        "    try:\n",
        "        neg_schema = {\n",
        "            \"negative_prompt\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Negative prompt to avoid unwanted artifacts in image generation\",\n",
        "                \"is_required\": True\n",
        "            }\n",
        "        }\n",
        "        neg_resp = client.evaluate_prompt(\n",
        "            prompt=(\n",
        "                f\"Captions: {captions[:3]}\\n\"\n",
        "                f\"Style Guide: {style_guide}\\n\"\n",
        "                \"From the above, produce a concise negative prompt that avoids common diffusion artifacts.\"\n",
        "            ),\n",
        "            system_message=(\n",
        "                \"You are an image quality guard. Given the inputs, output ONLY a single valid JSON object \"\n",
        "                \"with the key \\\"negative_prompt\\\" whose value is a concise string avoiding common diffusion artifacts. \"\n",
        "                \"Do not include any explanation, extra keys, or surrounding text. Example format: \"\n",
        "                \"{\\\"negative_prompt\\\": \\\"blurry, low-res, watermark\\\"}.\"\n",
        "            ),\n",
        "            llm_name=\"GEMINI_2_FLASH\",\n",
        "            response_type=\"json\",\n",
        "            json_response_schema=neg_schema,\n",
        "            max_tokens=60,\n",
        "        )\n",
        "        neg_data = safe_parse_json(getattr(neg_resp, \"content\", \"\") or \"\", \"negative prompt\", client)\n",
        "        negative_prompt = neg_data.get(\"negative_prompt\", default_negative)\n",
        "        if not isinstance(negative_prompt, str) or not negative_prompt.strip():\n",
        "            raise ValueError(\"Invalid negative prompt\")\n",
        "    except Exception as e:\n",
        "        client.stream_message(f\"‚ö†Ô∏è Negative prompt generation failed: {e}. Using default.\\n\")\n",
        "        negative_prompt = default_negative\n",
        "\n",
        "    # ---- Per-caption prompt generation with retry and robust parsing ----\n",
        "    client.stream_message(\"üß† Generating image prompts for five captions...\\n\")\n",
        "    caption_prompts = []\n",
        "    captions5 = (captions + captions[:5])[:5]\n",
        "    for i, cap in enumerate(captions5, start=1):\n",
        "        image_prompt = \"\"\n",
        "        base_system_msg = (\n",
        "            \"You are a visual prompt engineer for viral Instagram content. Given the caption, topic, sentiment, summary, \"\n",
        "            \"and style guide, output ONLY a single valid JSON object with the key \\\"image_prompt\\\" whose value is a rich, \"\n",
        "            \"diffusion-ready image prompt. Do not include explanation, commentary, markdown, or extra fields. \"\n",
        "            \"Example format: {\\\"image_prompt\\\": \\\"Close-up, golden-hour lighting, dynamic angle, enthusiastic mood...\\\"}. \"\n",
        "            \"Include composition, lighting, camera angle, mood, color palette, and a thumbnail-style hook in the prompt.\"\n",
        "        )\n",
        "        prompt_input = (\n",
        "            f\"Caption: {cap}\\n\"\n",
        "            f\"Topic: {topic}\\n\"\n",
        "            f\"Sentiment: {sentiment}\\n\"\n",
        "            f\"Summary: {summary}\\n\"\n",
        "            f\"Style Guide: {style_guide}\\n\"\n",
        "        )\n",
        "\n",
        "        for attempt in range(1, 3):  # primary + one stricter retry\n",
        "            try:\n",
        "                system_msg = base_system_msg\n",
        "                if attempt > 1:\n",
        "                    system_msg += \" AGAIN: Output only the JSON object, no explanation.\"\n",
        "                response = client.evaluate_prompt(\n",
        "                    prompt=prompt_input,\n",
        "                    system_message=system_msg,\n",
        "                    llm_name=\"GEMINI_2_FLASH\",\n",
        "                    response_type=\"json\",\n",
        "                    json_response_schema={\"image_prompt\": {\"type\": \"string\", \"description\": \"Visual prompt\", \"is_required\": True}},\n",
        "                    max_tokens=180,\n",
        "                )\n",
        "                raw = getattr(response, \"content\", \"\") or \"\"\n",
        "                data = safe_parse_json(raw, f\"image prompt for caption {i}\", client)\n",
        "                image_prompt = data.get(\"image_prompt\", \"\").strip()\n",
        "                if not image_prompt:\n",
        "                    client.stream_message(f\"‚ö†Ô∏è Empty or invalid image_prompt for caption {i} on attempt {attempt}; raw output: {raw[:1500]}{'...(truncated)' if len(raw) > 1500 else ''}\\n\")\n",
        "                    raise ValueError(\"Empty image prompt\")\n",
        "                caption_prompts.append({\"caption\": cap, \"image_prompt\": image_prompt})\n",
        "                client.stream_message(f\"‚úÖ Prompt for caption {i} obtained on attempt {attempt}.\\n\")\n",
        "                break  # success\n",
        "            except Exception as e:\n",
        "                client.stream_message(f\"‚ö†Ô∏è Prompt gen fallback attempt {attempt} for caption {i}: {e}\\n\")\n",
        "                if attempt == 2:\n",
        "                    fallback = f\"{cap}, {style_guide}, engaging Instagram reel thumbnail, photorealistic\"\n",
        "                    caption_prompts.append({\"caption\": cap, \"image_prompt\": fallback})\n",
        "        # end caption loop\n",
        "\n",
        "    # ---- Image generation: 1 image per caption ----\n",
        "    client.stream_message(\"üñºÔ∏è Generating images...\\n\")\n",
        "    generated_images = []\n",
        "    image_data_list = []\n",
        "    for cp_idx, cp in enumerate(caption_prompts, start=1):\n",
        "        base_prompt = cp[\"image_prompt\"]\n",
        "        prompt_text = f\"{base_prompt}, trending Instagram reel thumbnail\"\n",
        "        guidance = 7.5\n",
        "        steps = 30\n",
        "        success = False\n",
        "        attempt = 0\n",
        "        while attempt < 2 and not success:\n",
        "            attempt += 1\n",
        "            try:\n",
        "                client.stream_message(f\"‚Üí Generating image for caption {cp_idx} (attempt {attempt})...\\n\")\n",
        "                image_bytes = call_hf_diffusion(\n",
        "                    prompt=prompt_text,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    guidance_scale=guidance,\n",
        "                    steps=steps,\n",
        "                )\n",
        "                filename = f\"img_{topic.replace(' ', '_')}_cap{cp_idx}.png\"\n",
        "                generated_images.append(Blob(contents=image_bytes, mime_type=\"image/png\", filename=filename))\n",
        "                image_data_list.append({\n",
        "                    \"caption_index\": cp_idx,\n",
        "                    \"caption\": cp[\"caption\"],\n",
        "                    \"prompt\": prompt_text,\n",
        "                    \"negative_prompt\": negative_prompt,\n",
        "                    \"filename\": filename,\n",
        "                    \"backend\": \"hf_inference\",\n",
        "                    \"tuned_params\": {\"guidance\": guidance, \"steps\": steps},\n",
        "                })\n",
        "                client.stream_message(f\"‚úÖ Generated {filename}\\n\")\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                client.stream_message(f\"‚úó Error generating image for caption {cp_idx}: {e}\\n\")\n",
        "                time.sleep(0.8)\n",
        "        if not success:\n",
        "            client.stream_message(f\"‚ö†Ô∏è Using fallback for caption {cp_idx}\\n\")\n",
        "            mock_png = (\n",
        "                b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x01\"\n",
        "                b\"\\x00\\x00\\x00\\x01\\x08\\x02\\x00\\x00\\x00\\x90wS\\xde\"\n",
        "                b\"\\x00\\x00\\x00\\tpHYs\\x00\\x00\\x0b\\x13\\x00\\x00\\x00\\x0b\"\n",
        "                b\"\\x13\\x01\\x00\\x9a\\x9c\\x18\\x00\\x00\\x00\\x12IDATx\"\n",
        "                b\"\\x9cc```\\x00\\x00\\x00\\x04\\x00\\x01\\xddCC\\x9e\\x00\"\n",
        "                b\"\\x00\\x00\\x00IEND\\xaeB`\\x82\"\n",
        "            )\n",
        "            filename = f\"img_{topic.replace(' ', '_')}_cap{cp_idx}_fallback.png\"\n",
        "            generated_images.append(Blob(contents=mock_png, mime_type=\"image/png\", filename=filename))\n",
        "            image_data_list.append({\n",
        "                \"caption_index\": cp_idx,\n",
        "                \"caption\": cp[\"caption\"],\n",
        "                \"prompt\": prompt_text,\n",
        "                \"negative_prompt\": negative_prompt,\n",
        "                \"filename\": filename,\n",
        "                \"backend\": \"mock\",\n",
        "                \"tuned_params\": {\"guidance\": guidance, \"steps\": steps},\n",
        "            })\n",
        "\n",
        "    # ---- Report ----\n",
        "    client.stream_message(\"üìù Building report...\\n\")\n",
        "    markdown_content = f\"# Image Generation Report: {topic}\\n\\n**Sentiment:** {sentiment}\\n\\n**Summary:** {summary}\\n\\n**Style Guide:** {style_guide}\\n\\n\"\n",
        "    markdown_content += \"## Caption-conditioned Prompts\\n\"\n",
        "    for idx, cp in enumerate(caption_prompts, 1):\n",
        "        markdown_content += f\"### Caption {idx}\\n- **Caption:** {cp['caption']}\\n- **Image Prompt:** {cp['image_prompt']}\\n\\n\"\n",
        "    markdown_content += \"\\n## Generated Images\\n\"\n",
        "    for img in image_data_list:\n",
        "        markdown_content += (\n",
        "            f\"### Caption {img['caption_index']}\\n\"\n",
        "            f\"- **Filename:** {img['filename']}\\n\"\n",
        "            f\"- **Prompt:** {img['prompt']}\\n\"\n",
        "            f\"- **Negative Prompt:** {img['negative_prompt']}\\n\"\n",
        "            f\"- **Backend:** {img['backend']}\\n\"\n",
        "            f\"- **Tuned Params:** {img['tuned_params']}\\n\\n\"\n",
        "        )\n",
        "\n",
        "    json_data = {\n",
        "        \"topic\": topic,\n",
        "        \"sentiment\": sentiment,\n",
        "        \"summary\": summary,\n",
        "        \"style_guide\": style_guide,\n",
        "        \"caption_prompts\": caption_prompts,\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"generated_images\": image_data_list,\n",
        "        \"total_images\": len(generated_images),\n",
        "    }\n",
        "\n",
        "    markdown_bytes = markdown_content.encode(\"utf-8\")\n",
        "    json_bytes = json.dumps(json_data, indent=2).encode(\"utf-8\")\n",
        "\n",
        "    # ---- Return first five images ----\n",
        "    padded = generated_images[:5]\n",
        "    while len(padded) < 5:\n",
        "        pad_idx = len(padded) + 1\n",
        "        mock_png = (\n",
        "            b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x01\"\n",
        "            b\"\\x00\\x00\\x00\\x01\\x08\\x02\\x00\\x00\\x00\\x90wS\\xde\"\n",
        "            b\"\\x00\\x00\\x00\\tpHYs\\x00\\x00\\x0b\\x13\\x00\\x00\\x00\\x0b\"\n",
        "            b\"\\x13\\x01\\x00\\x9a\\x9c\\x18\\x00\\x00\\x00\\x12IDATx\"\n",
        "            b\"\\x9cc```\\x00\\x00\\x00\\x04\\x00\\x01\\xddCC\\x9e\\x00\"\n",
        "            b\"\\x00\\x00\\x00IEND\\xaeB`\\x82\"\n",
        "        )\n",
        "        filename = f\"pad_{topic.replace(' ', '_')}_{pad_idx}.png\"\n",
        "        padded.append(Blob(contents=mock_png, mime_type=\"image/png\", filename=filename))\n",
        "\n",
        "    client.stream_message(f\"‚úÖ Image generation complete! Generated {len(generated_images)} images.\\n\")\n",
        "\n",
        "    return AgentResponse(\n",
        "        image1=padded[0],\n",
        "        image2=padded[1],\n",
        "        image3=padded[2],\n",
        "        image4=padded[3],\n",
        "        image5=padded[4],\n",
        "        topic=topic,\n",
        "        sentiment=sentiment,\n",
        "        total_images=len(generated_images),\n",
        "        markdown_file=Blob(\n",
        "            contents=markdown_bytes,\n",
        "            mime_type=\"text/markdown\",\n",
        "            filename=f\"imagegen_report_{topic.replace(' ', '_')}.md\",\n",
        "        ),\n",
        "        json_file=Blob(\n",
        "            contents=json_bytes,\n",
        "            mime_type=\"application/json\",\n",
        "            filename=f\"imagegen_data_{topic.replace(' ', '_')}.json\",\n",
        "        ),\n",
        "    )\n"
      ]
    }
  ]
}
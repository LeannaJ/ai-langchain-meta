{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZExJ8xVhVgyn",
        "outputId": "c5808567-1f7d-4fee-832d-d0203d98458d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 unique subreddits. Here are the top 20:\n",
            "  1. interestingasfuck (5 posts)\n",
            "  2. facepalm (5 posts)\n",
            "  3. MadeMeSmile (4 posts)\n",
            "  4. politics (4 posts)\n",
            "  5. meirl (4 posts)\n",
            "  6. MurderedByWords (4 posts)\n",
            "  7. SipsTea (4 posts)\n",
            "  8. pics (4 posts)\n",
            "  9. clevercomebacks (4 posts)\n",
            " 10. Fauxmoi (4 posts)\n",
            " 11. WhitePeopleTwitter (4 posts)\n",
            " 12. worldnews (4 posts)\n",
            " 13. AITAH (4 posts)\n",
            " 14. europe (4 posts)\n",
            " 15. Unexpected (4 posts)\n",
            " 16. shitposting (4 posts)\n",
            " 17. whenthe (4 posts)\n",
            " 18. CuratedTumblr (4 posts)\n",
            " 19. anime_irl (4 posts)\n",
            " 20. cats (4 posts)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from requests.auth import HTTPBasicAuth\n",
        "from collections import Counter\n",
        "\n",
        "# ── 1. Fill in your app credentials ───────────────────────────────────────────\n",
        "CLIENT_ID     = \"77pr5LsRNFJ8yATCEcu-ZQ\"\n",
        "CLIENT_SECRET = \"6YIdUdDDgr8IteWLz1vAVhLekT4mQQ\"\n",
        "USER_AGENT    = \"MyApp/0.1 by YOUR_REDDIT_USERNAME\"\n",
        "\n",
        "# ── 2. Get a bearer token via client-credentials grant ────────────────────────\n",
        "auth = HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)\n",
        "data = {\"grant_type\": \"client_credentials\"}\n",
        "headers = {\"User-Agent\": USER_AGENT}\n",
        "token_res = requests.post(\n",
        "    \"https://www.reddit.com/api/v1/access_token\",\n",
        "    auth=auth, data=data, headers=headers\n",
        ").json()\n",
        "bearer = token_res[\"access_token\"]\n",
        "\n",
        "# ── 3. Page through r/all/hot, 100 posts at a time ───────────────────────────\n",
        "hot_url = \"https://oauth.reddit.com/r/all/hot\"\n",
        "headers[\"Authorization\"] = f\"bearer {bearer}\"\n",
        "\n",
        "after = None\n",
        "counts = Counter()\n",
        "MAX_PAGES = 20       # 20 pages × 100 posts = 2 000 posts total\n",
        "for page in range(MAX_PAGES):\n",
        "    params = {\"limit\": 100}\n",
        "    if after:\n",
        "        params[\"after\"] = after\n",
        "\n",
        "    resp = requests.get(hot_url, headers=headers, params=params)\n",
        "    resp.raise_for_status()\n",
        "    listing = resp.json()[\"data\"]\n",
        "\n",
        "    posts = listing[\"children\"]\n",
        "    if not posts:\n",
        "        break\n",
        "\n",
        "    # tally each post’s subreddit\n",
        "    for post in posts:\n",
        "        counts[post[\"data\"][\"subreddit\"]] += 1\n",
        "\n",
        "    after = listing.get(\"after\")\n",
        "    if not after:\n",
        "        break\n",
        "\n",
        "# ── 4. Extract the top 1 000 subreddits by post-count ─────────────────────────\n",
        "top_1000 = [sub for sub, _ in counts.most_common(1000)]\n",
        "\n",
        "# ── 5. Done! Print or save as you like ───────────────────────────────────────\n",
        "print(f\"Found {len(top_1000)} unique subreddits. Here are the top 20:\")\n",
        "for i, sub in enumerate(top_1000[:20], 1):\n",
        "    print(f\"{i:>3}. {sub} ({counts[sub]} posts)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from requests.auth import HTTPBasicAuth\n",
        "import pandas as pd\n",
        "\n",
        "# ── 1. Your Reddit app credentials ────────────────────────────────────────────\n",
        "CLIENT_ID     = \"77pr5LsRNFJ8yATCEcu-ZQ\"\n",
        "CLIENT_SECRET = \"6YIdUdDDgr8IteWLz1vAVhLekT4mQQ\"\n",
        "USER_AGENT    = \"MyApp/0.1 by YOUR_REDDIT_USERNAME\"\n",
        "\n",
        "# ── 2. Get an OAuth bearer token ─────────────────────────────────────────────\n",
        "auth = HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)\n",
        "data = {\"grant_type\": \"client_credentials\"}\n",
        "headers = {\"User-Agent\": USER_AGENT}\n",
        "\n",
        "token_res = requests.post(\n",
        "    \"https://www.reddit.com/api/v1/access_token\",\n",
        "    auth=auth, data=data, headers=headers\n",
        ")\n",
        "token_res.raise_for_status()\n",
        "bearer = token_res.json()[\"access_token\"]\n",
        "\n",
        "# ── 3. Page through r/all/hot until you have 1 000 posts ────────────────────\n",
        "hot_url = \"https://oauth.reddit.com/r/all/hot\"\n",
        "headers[\"Authorization\"] = f\"bearer {bearer}\"\n",
        "\n",
        "all_posts = []\n",
        "after = None\n",
        "\n",
        "while len(all_posts) < 1000:\n",
        "    to_fetch = min(100, 1000 - len(all_posts))\n",
        "    params = {\"limit\": to_fetch}\n",
        "    if after:\n",
        "        params[\"after\"] = after\n",
        "\n",
        "    res = requests.get(hot_url, headers=headers, params=params)\n",
        "    res.raise_for_status()\n",
        "    data = res.json()[\"data\"]\n",
        "    children = data.get(\"children\", [])\n",
        "    if not children:\n",
        "        break\n",
        "\n",
        "    all_posts.extend(children)\n",
        "    after = data.get(\"after\")\n",
        "    if not after:\n",
        "        break\n",
        "\n",
        "# trim just in case\n",
        "all_posts = all_posts[:1000]\n",
        "\n",
        "# ── 4. Normalize and save to CSV ─────────────────────────────────────────────\n",
        "# Extract the inner \"data\" dict from each post\n",
        "posts_data = [post[\"data\"] for post in all_posts]\n",
        "\n",
        "# Flatten into a DataFrame\n",
        "df = pd.json_normalize(posts_data)\n",
        "\n",
        "# Write out to CSV\n",
        "output_path = \"reddit_hot_posts.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Saved {len(df)} posts to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iijS_Ml9iLYv",
        "outputId": "633c6e6e-61b8-43c2-d0bb-836052c55ff6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1000 posts to reddit_hot_posts.csv\n"
          ]
        }
      ]
    }
  ]
}